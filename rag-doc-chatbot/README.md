# ğŸ“„ Single-Document RAG Chatbot (FastAPI + FAISS + LangChain + Streamlit)

A turnkey template to build a **live chatbot** that answers questions **only** from one (or a few) documents â€” perfect for things like a healthcare company's patient info sheet where queries like _"What are your business hours?"_ must be answered from the doc.

---

## âœ¨ Features
- **RAG pipeline** with FAISS vector store and Sentence-Transformer embeddings (local, free).
- **FastAPI** backend with `/chat` and `/rebuild` endpoints.
- **Streamlit** chat UI.
- **Doc ingestion** supports **PDF**, **DOCX**, **TXT**.
- **Grounded answers**: if the info isnâ€™t in your doc, the bot says it doesnâ€™t know.
- Minimal vendor lock-in; LLM is OpenAI by default but you can swap it.

---

## ğŸ§± Project Structure
```
rag-doc-chatbot/
â”œâ”€ backend/
â”‚  â””â”€ app.py            # FastAPI server (RAG pipeline + endpoints)
â”œâ”€ data/
â”‚  â””â”€ sample_clinic_info.txt  # Example doc with business hours
â”œâ”€ storage/             # FAISS index gets stored here after ingestion
â”œâ”€ ingest.py            # Load & chunk docs, build FAISS index
â”œâ”€ streamlit_app.py     # Simple chat UI
â”œâ”€ requirements.txt
â””â”€ .env.example
```

---

## ğŸš€ Quickstart

> Prereqs: Python 3.10+ recommended.

```bash
# 1) Create and activate a venv (recommended)
python -m venv .venv
source .venv/bin/activate   # Windows: .venv\Scripts\activate

# 2) Install dependencies
pip install -r requirements.txt

# 3) Add your docs
# Put a PDF/DOCX/TXT into ./data (sample already provided)

# 4) (Optional) Configure environment
cp .env.example .env
# - Put OPENAI_API_KEY in .env (for better generations)
# - Tweak CHUNK_SIZE/OVERLAP/TOP_K if needed

# 5) Build the vector index
python ingest.py

# 6) Start the backend API
uvicorn backend.app:app --reload --host 0.0.0.0 --port 8000

# 7) Start the UI (new terminal)
streamlit run streamlit_app.py
```

Open Streamlit URL shown in the terminal, ask: **"What are the business hours?"**

---

## ğŸ§  How the RAG Flow Works

```
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚    Your Document   â”‚  (PDF/DOCX/TXT)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Load & Split
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Text Chunks       â”‚  (Recursive splitter)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Embed (local MiniLM)
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  FAISS Vector DB   â”‚  (./storage)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Retrieve top-k
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Prompt Template   â”‚  (Context + Question)
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚ Generate (OpenAI or other)
                      â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  Grounded Answer   â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why these choices?**
- **RecursiveCharacterTextSplitter**: robust across formats; preserves local coherence.
- **Sentence-Transformers (MiniLM)**: fast, small, solid recall; no API cost.
- **FAISS**: battle-tested local vector index; easy `save_local/load_local`.
- **ChatOpenAI (OpenAI)**: strong instruction-following for concise, accurate answers; you can swap it.

**Alternatives you can plug in:**
- **Embeddings**: `text-embedding-3-small` (OpenAI), `bge-small-en`, `all-mpnet-base-v2`.
- **Vector DB**: Chroma, Qdrant, Weaviate, Pinecone.
- **Chunking**: token-based splitters, semantic split (e.g., `TokenTextSplitter` or `SemanticChunker`).

---

## ğŸ”„ Updating the Knowledge
- Replace files in `./data/` with your latest document(s).
- Run `python ingest.py` (or click **Rebuild Index** in the Streamlit sidebar).
- Thatâ€™s it â€” the bot starts answering from the new content.

---

## ğŸ”’ Guardrails & Tips
- The system prompt forces **"answer only from context"**. The bot will say _"I don't know"_ if info is missing.
- Keep documents **clean & up-to-date** (e.g., a single source of truth per clinic).
- For hours/phones/emails, ensure they appear **verbatim** in the doc â€” the bot will quote them.

---

## ğŸ” Switching the LLM (Optional)
By default we use OpenAI via `langchain-openai`. To switch:
- **Ollama (local)**: Replace `get_llm()` in `backend/app.py` with an Ollama LLM wrapper.
- **Groq**: Use `langchain-groq` and set `GROQ_API_KEY`.
- **Transformers pipeline (local)**: Use `langchain`'s `HuggingFacePipeline` with an instruct model.

---

## ğŸ§ª Test Prompts
- "What are the business hours?"
- "Do you offer lab tests on Saturday? Until what time are samples accepted?"
- "Which insurers are accepted and how to use cashless?"
- "What is the emergency after-hours contact?"

---

## ğŸ Common Issues
- **`Vector store not found`**: Run `python ingest.py` after placing docs in `./data/`.
- **OpenAI key missing**: Add `OPENAI_API_KEY` to `.env` or remove OpenAI usage and switch to a local LLM.
- **Poor recall**: Increase `TOP_K`, reduce `CHUNK_SIZE`, increase `CHUNK_OVERLAP`.
- **Duplicates**: Keep `./data` tidy; re-run ingestion after changes.

---

## ğŸ“¦ Deploy (quick hints)
- **Backend**: `uvicorn` behind Nginx or run as a service (systemd) or Docker.
- **UI**: Streamlit can be containerized or replaced with a React frontend calling the same API.
- **Secrets**: Mount `.env` with proper permissions.

---

Happy building! ğŸ’ª
